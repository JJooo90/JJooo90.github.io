---
title:  "[study] 공부할 계획 1단계 "
excerpt: "CNN 공부 내용 정리"

categories:
  - study
tags:
  - [plan]

toc: true
toc_sticky: true
 
date: 2024-06-21
last_modified_at: 2024-06-21
---

# 1. 공부할 것 리스트
## 1. ANN
### 1. EarlyStopping
### 2. Sequential 모델

## 2. CNN
  1. 합성곱 층(Convolutional Layer)  
    - 필터(filter) : 입력 이미지의 작은 패치와 컨볼루션 연산을 수행하는 매개변수, 작은 크기의 행렬로 이미지 전체를 스캔하며 활성화 맵을 생성한다.  
    - Stride : 필터가 입력 이미지를 이동하는 간격 (클수록 출력 이미지가 작아진다.)  
    - Padding : 입력 이미지 가장자리
  2. 풀링 층(Pooling Layer)  
    - Max Pooling : 주어진 영역 내에서 가장 큰 값을 선택하여 특징을 추출하고, 모델의 복잡도를 낮춘다.  
    - Average Pooling : 평균 값을 계산하여 특징을 추출한다.

  - 완전연결 층(Fully Connected Layer) 마지막 단계

  - 활성화 함수(Activation Function)
### 1. VGG16
  이미지 분류 모델 중 하나 CNN 구조를 사용하는 모델

  - 합성곱 레이어 
    1. 총 13 개의 합성곱 레이어 구성되어 있다.
    2. 작은 크기의 필터를 사용하여 특징을 추출한다. (3x3)
    3. 필터 수가 점점 증가하여 특징을 깊이 있게 학습할 수 있다. (32 -> 64 -> 128 -> 256 -> 512 ...)
    4. 합성곱 연산을 통해 이미지에서 로컬 특징을 추출한다.

  - 풀링 레이어 : 최대 풀링(Max Pooling) 레이어를 사용하여 특징 맵의 크기를 줄인다. 매개변수의 개수를 감소시키고 특징에 대한 불변성을 높일 수 있다.

  - 패딩 : 입력 이미지의 크기를 유지하기 위해 합성곱 연산 시 패딩을 사용한다. 연산 후에도 feature map의 크기가 입력과 동일하게 유지된다.

  - Stride : 대부분 합성곱 레이어에서 stride 크기를 1로 설정한다. feature map의 크기 변화를 최소화한다. (공간정보 유지)

  - 활성화 함수 : ReLU 함수를 사용하여 비선형성을 도입한다.
  
  > `가중치 고정 이유`  
    - 대규모 데이터 셋에서 학습된 모델은 일반적으로 강력한 특징 추출 능력을 가지고 있다. (더 이상 학습할 필요가 없다.)  
    - 데이터 셋이 작은 경우 사전 학습 모델의 가중치를 고정하면 과적합을 방지할 수 있다.  
    - 학습 속도가 빠르고, 메모리 사용량이 줄어든다.

  > `데이터 증강(Data Augmentation) -> 모델 향상`  
```
data = ImageDataGenerator(
    featurewise_center = False, # 전체 데이터셋의 평균을 0으로 만들건지?
    samplewise_center = False, # 각 샘플의 평균을 0으로 만들건지?
    featurewise_std_normalization=False, # 전체 데이터 셋의 표준편차로 정규화를 할건지?
    samplewise_std_normalization=False, # 각 샘플의 표준편차로 정규화를 할건지?
    zca_whitening=False, # ZCA 백색화를 할 것인지?
    rotation_range=5, # 이미지를 범위내에서 무작위로 회전 (5도 내에서 무작위 회전)
    zoom_range=0.1, # 이미지를 범위내에서 무작위로 확대/축소 (10% 범위 내에서)
    width_shift_range=0.1, # 이미지를 수평으로 범위내에서 이동 
    height_shift_range=0.1, # 이미지를 수직으로 범위내에서 이동
    horizontal_flip=False, # 이미지를 수평으로 뒤집을지?
    vertical_flip=False # 이미지를 수직으로 뒤집을지?
)
```


  > 잔차 블록 (Residual Block) : 신경망의 깊이를 증가시켜주면서 학습 성능을 향상시킨다. (ResNet)
  입력을 그대로 전달하는 지름길을 추가하여 신경망이 깊어질수록 발생할 수 있는 기울기 소실 문제를 완화시킨다.

  입력 특징 맵, 합성곱 레이어, 스킵 연결(지름길), 출력 y = F(x) + x    [ F(x) : 합성곱 층을 지나온 변환 함수 ]

  > `Xception (Extreme Inception)` : Depthwise Separable Convolution 기법을 사용한 합성곱 계층  
    공간적 특성과 채널 간의 특성을 분리하여 처리하는 특징을 가지고 있다.  
    Depthwise Separable Convolution  
      1. Depthwise Convolution  
         - 각 입력 채널에 별도의 합성곱 필터를 적용한다. (공간적 특징을 추출)  
         - (H x W x C) 크기의 이미지가 입력되었다면, C에 대해 다른 필터를 적용  
      2. Pointwise Convolution  
         - 1 x 1 합성곱을 사용하여 채널간의 상호작용 저장한다.  
         - Depthwise Convolution의 출력과 결합하여 새로운 특징맵을 만든다.  
  
    * 장점  
           - 필터 수와 계산량을 줄일 수 있기 때문에 계산 비용을 크게 줄일 수 있다.  
           - 기울기 소실 문제를 해결하고 학습을 안정화시킬 수 있다.  
           - 깊고 넓은 신경망 구조로 복잡한 데이터 또한 잘 학습할 수 있다.  

    * 구성요소
      1. Entry Flow
      - 모델의 시작 부분으로 기본적인 특징들을 추출한다. 
      (Depthwise Separable Convolution과 Max Pooling 레이어로 구성)

      2. Middle Flow
      - 입력 특징을 더 깊게 학습한다. 
      (동일한 구조의 반복되어 있는 Depthwise Separable Convolution 블록들로 구성)
      - 위 블록들은 잔차연결을 통해 입력과 출력을 연결하여 학습 안정화를 돕는다.

      3. Exit Flow
      - 최종 특징을 추출하고 분류를 수행한다. 
      (Depthwise Separable Convolution, Global Average Pooling, Fully Connected 레이어로 구성)

###