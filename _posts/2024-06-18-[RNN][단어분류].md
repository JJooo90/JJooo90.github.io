---
title:  "[RNN][단어분류] "
excerpt: "github Blog  Minimal-Mistakes 테마의 디렉터리 구조."

categories:
  - Blog
tags:
  - [Blog, github, git]

toc: true
toc_sticky: true
 
date: 2024-06-18
last_modified_at: 2024-06-18
---

### 용어 설명
bag of Words : 문서 내에 있는 모든 단어의 집합  
Stop Words : 의미를 전달하지 않는 고빈도 일반 단어 (the, of)  
TF : 단어 빈도  
IDF : 전체 문서 수를 해당 단어가 포함된 문서 수로 나눈 값의 로그
TF-IDF 벡터 라이저 : TF * IDF  
Word Embedding : 단어를 수치 벡터 형태로 표현한 것

### 분석 방법
1. One-Hot 백터 (단순한 표기법) : 유사성 전혀 판달 할 수 없다. [1 , 0 , 0]
2. SVD : 유사성을 판단할 수 있는 단어 임베딩을 찾는 것 (데이터 셋에서 단어 동시 발생 횟수를 매트릭스에 누적한다.)  
   LSA(잠재 의미 분석) : 데이터 셋에 포함된 단어 간의 관계를 분석하여 문서와 단어에 관련된 개념을 생성하는 기술  
   문서당 단어 수를 포함하는 매트릭스를 만들고, SVD를 사용하여 열 사이의 유사성 구조를 유지하면서 행의 수를 줄인다.

#### 매트릭스
word-documnet Matrix(여기 확인 필요) : 관련된 단어는 동일한 문서에서 자주 나타날 것이다라고 가정한다.  
    단어 i가 문서 j에 나타날 때마다, Xij의 1를 더한다  
    window-based co-occurrence Matrix (윈도우 기반 동시 발생) : 단어의 동시 발생을 저장하는 매트릭스

1. Word2Vec : 한번에 하나의 반복을 학습하고 주어진 문맥에서 단어의 확률을 인코딩할 수 있는 모델이다. (단어 벡터를 매개변수로 한다)  
    CBOW(Continuous bag-of-words) : 주변 단어로부터 중심 단어를 예측  
    Skip-gram : 중심 단어로부터 주변 단어를 예측

2. GloVe : SVD 접근 방법과 Skip-gram 모델을 결합한 모델이다. 단어 i가 단어 j에 포함되어 있는 문맥에서 나타날 확률(Pij = Xij / Xi)  
    윈도우 기반 매트릭스(CBOW)를 만들고 동시 발생 확률의 비율을 계산한다.


### 언어 모델 : 특정 순서로 단어가 나타날 확률 계산한다.
1.  N-Gram 언어 모델 : 이전 n개의 단어를 보고 특정 순서로 단어가 나타날 확률을 계산한다.
2.  RNN : 이전의 모든 단어에 대한 조건부 언어 모델을 만든다.  
    시퀀스 데이터를 처리하기 위한 신경망 구조  
    이전 시간의 정보를 현재 시간의 계산에 반영하여 순차적인 데이터 처리에 적합하다. 


#### RNN
- RNN은 가장 기본적인 시퀀스(Sequence) 모델이다.
- 시퀀스 모델은 입력과 출력을 시퀀스 단위로 처리하는 모델을 말한다.  
예를 들어 단어 시퀀스는 문장을 의미한다.

1. RNN(Recurrent Neural Network) 개념
  1. 순환구조 : 입력 데이터 시퀀스를 순차적으로 처리하면서 각 단계의 출력을 다음 단계의 입력으로 사용한다. 네트워크가 이전 시퀀스의 정보를 기억하고 사용할 수 있도록 한다.

  2. 은닉 상태 : RNN은 은닉 상태 ht를 유지한다. 은닉상태는 현재 시점의 입력 xt과 이전 시점 ht-1의 은닉 상태를 기반으로 갱신된다.

  3. 출력

