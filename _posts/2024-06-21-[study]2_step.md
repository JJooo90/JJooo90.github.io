---
title:  "[study] 공부할 계획 2단계 "
excerpt: "NLP 공부 내용 정리"

categories:
  - study
tags:
  - [plan]

toc: true
toc_sticky: true
 
date: 2024-06-21
last_modified_at: 2024-06-21
---

# 1. 공부할 것 리스트
## 1. NLP
    1. 의미론적, 통사적 전달 방법과 표현 학습
      - NLP (자연어 처리)에서 중요한 문제 단어를 모델의 입력으로 어떻게 표현할 것인가? queen이라는 단어는 king, cat

      - 우리는 단어들을 어떠한 특정 단어 공간에서 벡터로 인코딩하여야 한다. (모든 언어의 의미를 인코딩 할 수 있는 N 차원 공간)

      - 텍스트처리 4단계
       1. 로딩(Loading): 텍스트를 문자열로 메모리에 로드한다.
       2. 클리닝(Cleaning) : 텍스트를 정리하고, 철자 오류 수정하고, 이모지 및 특수기호 등을 제거한다.
       3. 토큰화(Tokenization) : 문자열을 단어 또는 문자와 같은 토큰으로 분할한다.
       4. 벡터화(Vectorization) : 데이터를 모델에 쉽게 입력할 수 있도록 모든 토큰을 벡터로 매핑한다.

      - 용어 설명
        Bag of Words : 문서 내에 있는 모든 단어의 집합
        Stop Words : 의미를 전달하지 않는 고빈도 일반 단어 (the, of)
        TF : 단어 빈도
        IDF : 전체 문서 수를 해당 단어가 포함된 문서 수로 나눈 값의 로그
        TF-IDF 벡터라이저 : TF * IDF
        Word Embedding : 단어를 수치 벡터 형태로 표현한 것

    2. 분석 방법
       1. One-Hot 벡터 (단순한 표기법) : 유사성 전혀 판단할 수 없다. [1, 0, 0]

       2. SVD : 유사성을 판단할 수 있는 단어 임베딩을 찾는 것 (데이터 셋에서 단어 동시 발생 횟수를 매트릭스에 누적한다.)
       LSA (잠재 의미 분석) : 데이터 셋에 포함된 단어 간의 관계를 분석하여 문서와 단어에 관련된 개념을 생성하는 기술
                           문서당 단어 수를 포함하는 매트릭스를 만들고, SVD를 사용하여 열 사이의 유사성 구조를 유지하면서 행의 수를 줄인다.

       * '매트릭스'
          - Word-documnet Matrix : 관련된 단어는 동일한 문서에서 자주 나타날 것이다라고 가정한다. 단어 i가 문서 j에 나타날 때마다 Xij의 1을 더한다.  
          - Window-based co-occurrence Matrix (윈도우 기반 동시발생) : 단어의 동시발생을 저장하는 매트릭스

       3. Word2Vec : 한번에 하나의 반복을 학습하고 주어진 문맥에서 단어의 확률을 인코딩할 수 있는 모델이다. (단어 벡터를 매개변수로 한다)
           - CBOW (Continuous bag-of-words) : 주변 단어로부터 중심 단어를 예측
           - Skip-gram : 중심 단어로부터 주변 단어를 예측

       4. GloVe : 
           - SVD 접근 방법과 Skip-gram 모델을 결합한 모델이다. 단어 i가 단어 j가 포함되어 있는 문맥에서 나타날 확률 (Pij = Xij / Xi)
           - 윈도우 기반 매트릭스를 만들고 동시 발생 확률의 비율을 계산한다.

## 언어 모델 : 특정 순서로 단어가 나타날 확률 계산한다.

    1. N-gram 언어 모델 : 이전 n개의 단어를 보고 특정 순서로 단어가 나타날 확률 계산한다.

    2. RNN : 이전의 모든 단어에 대한 조건부 언어 모델을 만든다. 시퀸스 데이터를 처리하기 위한 신경망 구조 
    이전 시간의 정보를 현재 시간의 계산에 반영하여 순차적인 데이터 처리에 적합하다.

## RNN
1. 순환구조 : 입력 데이터 시퀸스를 순차적으로 처리하면서 각 단계의 출력을 다음 단계의 입력으로 사용한다. 네트워크가 이전 시퀀스의 정보를 기억하고 사용할 수 있도록 한다.

2. 은닉 상태 : RNN은 은닉 상태 ht를 유지한다. 은닉 상태는 현재 시점의 입력xt과 이전 시점ht-1의 은닉 상태를 기반으로 갱신된다.
ht = σ(Whh * ht-1 + Wxh * xt + bh)

3. 출력 : 은닉 상태에서 계산된다.
yt = σ(Why * ht + by)

![image.png](attachment:98aeb657-7214-41db-a82e-a65c97671a6b.png)

 
> ''   
    - Bag of Words (BoW) : 구조나 순서를 무시하고 단어의 빈도에 초점을 맞춰서 텍스트 데이터를 수치화하여 모델 입력을 할 수 있도록 하는 기법  
    - 단어 집합(Vocabulary) : 단어 빈도 (Word Frequency) -> 텍스트 데이터를 벡터로 표현하였다.
    - 바이그램(Bigram) : 연속된 두 단어 쌍 i write rewrite and [UNK] rewrite again -> i write, write rewrite, rewrite and ...  
    - 이진 인코딩 : 바이그램이 존재하면 1, 존재하지 않으면 0


---
---


## 1. 트랜스포머 아키텍처 : 셀프 어텐션을 통해 문장의 맥락을 포착하는데 사용되는 모델 (문장 속 모든 단어의 관련성과 맥락을 학습)

> 셀프 어텐션 (Self-Attention) : 시퀸스의 각 단어가 다른 단어들과 어떻게 관련되어 있는 계산하는 메커니즘
   1. 쿼리, 키, 값 벡터를 생성
   2. 어텐션 스코어 : 각 단어의 쿼리 벡터와 모든 단어의 키 벡터를 곱한다.
   3. 소프트맥스 : 어텐션 스코어를 소프트맥스 함수에 통과시켜 가중치를 만든다.
   4. 어텐션 값 : 가중치를 값 벡터에 곱하여 가중합을 계산한다.

> 인코더-디코더 구조
   - 인코더
   1. 입력 임베딩 : 입력된 각 단어를 특정 차원의 벡터로 변환한다.
   2. 포지셔널 인코딩 : 단어의 위치 정보를 추가하여 순서의 의미를 반영한다.
   3. 인코더 레이어 : 각 레이어는 셀프 어텐션과 피드포워드 네트워크로 이루어져있다.

> - 디코더
   1. 출력 임베딩 : 생성된 각 단어를 특정 차원의 벡터로 변환한다.
   2. 포지셔널 인코딩 : 위치 정보 추가
   3. 디코더 레이어 : 각 레이어는 셀프 어텐션과 피드포워드 네트워크, 인코더-디코더 어텐션로 이루어져있다.
--- 
> 멀티 헤드 어텐션 : 셀프 어텐션의 여러 헤드를 병렬로 사용하는 방식 (각각 헤드가 다른 부분을 처리하여 다양한 표현을 학습할 수 있다.)
   - 다중 헤드 분할 : 입력을 여러 개의 서브스페이스로 분할한다.
   - 개발 어텐션 계산 : 각 서브스페이스에서 셀프 어텐션을 수행한다.
   - 병합 : 각 헤드의 출력을 병합하여 다음 레이어로 전달한다.
   - 트랜스포머 인코더 순서 : 멀티 헤드 셀프 어텐션 -> 잔차 연결 -> 정규화 -> 피드포워드 신경망 -> 잔차 연결 -> 정규화

---

> 위치 인코딩 (Position Encoding) : 각 토큰의 위치 정보를 인코딩하여 입력에 추가하는 것 (모델이 입력 시퀸스의 순서 정보를 학습)
트랜스포머 모델은 순차적인 정보 처리를 위한 재귀적 구조가 존재하지 않는다.

   1. 고정 위치 인코딩 (Fixed Position Encoding) : 계산이 간단하고 구현하기 쉽지만 입력 시퀸스의 길이가 고정되어야 한다.
      - 사인 함수와 코사인 함수를 사용하여 각 토큰의 위치정보를 인코딩한다.
      PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
      PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

   2. 학습 가능한 위치 인코딩 (Learned Position Encoding) : 입력 시퀸스의 길이는 제한이 없지만 별도의 학습과정이 필요하다.
      - 위치 정보를 학습 가능한 매개변수로 표현한다.


---
> BoW 모델 대신 언제 시퀸스 모델을 사용할까? -> 단어의 순서를 무시하고 단어의 빈도만 처리하는 것 대신 단어의 순서를 고려해야할 때?

   1. 문맥을 고려해야 할 때 : dog bites man, man bites dog
   2. 언어 모델링 : 다음 단어를 예측하거나 텍스트를 생성할 때
   3. 긴 의존성 : RNN 트랜스포머는 긴 시퀸스 내에서 멀리 떨어진 단어 간의 의존성 파악할 수 있다.



---
> 시퀸스-투-시퀸스(Sequence-to-Sequence, Seq2Seq) : 하나 시퀸스를 입력받아 또 다른 시퀸스를 출력하는 모델

1. 인코더 (Encoder) : 입력 시퀸스를 고정된 길이의 컨텍스트 벡터(Context Vector)로 변환한다.
   - RNN, LSTM, GRU와 같은 순환 신경망이 사용된다. 인코더는 입력 시퀸스를 시간 단계별로 처리하여 각 단계의 은닉 상태를 생성한다.
   - 마지막 은닉 상태에서 입력 시퀸스의 정보를 요약한 컨텍스트 벡터로 사용된다.

2. 디코더 (Decoder) : 인코더에서 생성된 컨텍스트 벡터를 사용하여 출력 시퀸스를 생성한다.
   - RNN, LSTM, GRU와 같은 순환 신경망이 사용된다.
   - 인코더의 마지막 은닉 상태를 초기 상태로 사용하고 이전 단계의 출력을 다음 시간 단계의 입력으로 사용한다.
- 